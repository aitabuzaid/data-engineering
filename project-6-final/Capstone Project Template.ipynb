{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Lake for US Immigration Data Using Spark \n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project uses immigration data to the United States, combined with three other data sources, namely, historical averages of weather temperatures, airports in the US, and demographics of US cities in order to create a data lake through Spark that is ready for analysis when required. The project will provide a data model in a star schema with one fact and three dimensional tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import column as col\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The project will combine four data sources: I94 Immigration Data, World Temperature Data, US City Demographic Data, and Airport Code Table. I will use Spark in order to build a data lake and model the data in a star schema, with the I94 immigration data as the fact table and all other data sources as the dimension tables. The tables will be saved as parquet files as the end solution will be a set of parquet files that can be loaded whenever a new analysis is needed by the Data Science team. The only tool that will be used in this project is Apache Spark. Amazon EMR and Apache Airflow will not be used in this project, however, the project can be extended later on to include these technologies once the processing requirements increase. \n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "The fact table is the immigration data from the US National Tourism and Trade Office, detailing the respondents information (people travelling into the US), while the dimension tables are as follows: the first provides the average temperature for cities in the world and it is provided by Kaggle datasets, the second provides detailed information about city airports in the US, and it is provided from DataHub, while the last provides further details about demographics in US cities, and it is provided from OpenSoft. The three data sets will be joined through taking cities as the primary key.\n",
    "\n",
    "Sample queries that the final data model will be able to answer:\n",
    "\n",
    " -How does immigration differ in terms of visa type and the origin country? \n",
    " \n",
    " -Is there any effect of temperature on the number of people visiting a city? \n",
    " \n",
    " -What is the correlation between the type of airport and the number of people visiting a city? \n",
    " \n",
    " -Do people form certain geographical areas are more likely to visit US cities with certain demographics? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Exploring and Cleaning the Data\n",
    "The immigration data is the largest out of the four data sources, and it is the most challenging to clean. Based on the problems I want to solve and specified in part 1 of this project, I decided to drop eight columns that are either irrelevant or have a lot of missing values. The dropped columns and the number of non-null values in each are as follows: \n",
    "\n",
    "- visapost:  1215063\n",
    "- occup:        8126\n",
    "- entdepa:   3096075\n",
    "- entdepd:   2957884\n",
    "- entdepu:       392\n",
    "- matflag:   2957884\n",
    "- dtaddto:   3095836\n",
    "- insnum:     113708\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Loading the immigration data and creating the spark session\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_imm =spark.read.format('com.github.saurfang.sas.spark').load(fname)\n",
    "\n",
    "# selecting only the relavent columns for the project \n",
    "# and selected data model\n",
    "df_imm = df_imm.selectExpr([\"cicid AS  id\",\n",
    "                            \"i94yr AS  year\",\n",
    "                            \"i94mon AS month\",\n",
    "                            \"i94res AS origin_country_id\",\n",
    "                            \"i94port AS port_city_id\",\n",
    "                            \"arrdate AS arrival_date\",\n",
    "                            \"i94mode AS transport_mode\",\n",
    "                            \"depdate AS departure_date\",\n",
    "                            \"i94visa AS visa_purpose\",\n",
    "                            \"visatype AS visa_type\",\n",
    "                            \"biryear AS birth_year\",\n",
    "                            \"gender\",\n",
    "                            \"admnum AS admin_num\",\n",
    "                            \"airline AS airline_code\",\n",
    "                            \"fltno AS flight_number\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Moreover, the weather data is huge and has around 8 million records. Each record has the average monthly temperature for one city. The city spans more than 250 years and for cities from all over the world. We need to clean the data and group it so that it only captures monthly averages across all years and only for US cities. This will provide a way for us to join the temperature data with the immigration data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# loading the original world temperature data\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temp = spark.read.csv(fname, header=True)\n",
    "\n",
    "# Filtering temperature data by month (April) and country (United States)\n",
    "# Grouping the average of all years by city\n",
    "df_temp = df_temp.filter((df_temp.dt[6:2]==\"04\") & (df_temp.Country == \"United States\"))\\\n",
    "                 .groupBy(df_temp.City,df_temp.Latitude,df_temp.Longitude)\\\n",
    "                 .agg(F.avg(\"AverageTemperature\").alias(\"avg_temp\"))\\\n",
    "                 .withColumnRenamed(\"city\", \"city2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Finally, the airport and demographic data is clean already and it only requires column renaming in order to follow the same naming convention among all tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# loading the original airport data\n",
    "fname = 'airport-codes_csv.csv'\n",
    "df_airports = spark.read.csv(fname, header=True)\n",
    "\n",
    "# selecting only the relavent columns for the project \n",
    "# and selected data model\n",
    "df_airports = df_airports.selectExpr([\"iata_code\",\n",
    "                                      \"name\",\n",
    "                                      \"type\",\n",
    "                                      \"municipality AS city\",\n",
    "                                      \"iso_region AS region\",\n",
    "                                      \"iso_country AS country\",\n",
    "                                      \"continent\",\n",
    "                                      \"coordinates\",\n",
    "                                      \"gps_code\",\n",
    "                                      \"local_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# loading the original demographics data\n",
    "fname = 'us-cities-demographics.csv'\n",
    "\n",
    "# renaming column names to follow convention\n",
    "df_demographics = spark.read.csv(fname, header=True, sep=';')\\\n",
    "                       .withColumnRenamed(\"City\", \"city2\")\\\n",
    "                       .withColumnRenamed(\"State\", \"state\")\\\n",
    "                       .withColumnRenamed(\"State Code\", \"state_code\")\\\n",
    "                       .withColumnRenamed(\"Median Age\", \"median_age\")\\\n",
    "                       .withColumnRenamed(\"Male Population\", \"male_population\")\\\n",
    "                       .withColumnRenamed(\"Female Population\", \"female_population\")\\\n",
    "                       .withColumnRenamed(\"Total Population\", \"total_population\")\\\n",
    "                       .withColumnRenamed(\"Number of Veterans\", \"number_of_veterans\")\\\n",
    "                       .withColumnRenamed(\"Foreign-born\", \"foreign_born\")\\\n",
    "                       .withColumnRenamed(\"Average Household Size\", \"household_size\")\\\n",
    "                       .withColumnRenamed(\"Race\", \"race\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# join cleaned immigration data with airports data\n",
    "df_join = df_imm.join(df_airports, df_imm.city_id==df_airports.iata_code, 'left')\n",
    "\n",
    "# final immigration table\n",
    "df_imm = df_join.filter(df_join.iata_code.isNotNull())\\\n",
    "                .select([\"id\",\n",
    "                         \"year\",\n",
    "                         \"month\",\n",
    "                         \"origin_country_id\",\n",
    "                         \"city_id\",\n",
    "                         \"city\",\n",
    "                         \"arrival_date\",\n",
    "                         \"transport_mode\",\n",
    "                         \"departure_date\",\n",
    "                         \"visa_purpose\",\n",
    "                         \"visa_type\",\n",
    "                         \"birth_year\",\n",
    "                         \"gender\",\n",
    "                         \"admin_num\",\n",
    "                         \"airline_code\",\n",
    "                         \"flight_number\"])\n",
    "\n",
    "# final airports table\n",
    "df_airports = df_join.filter(df_join.iata_code.isNotNull())\\\n",
    "                     .select([\"iata_code\",\n",
    "                              \"name\",\n",
    "                              \"type\",\n",
    "                              \"city\",\n",
    "                              \"region\",\n",
    "                              \"country\",\n",
    "                              \"continent\",\n",
    "                              \"coordinates\",\n",
    "                              \"gps_code\",\n",
    "                              \"local_code\"])\\\n",
    "                     .dropDuplicates()\n",
    "\n",
    "# join final immigration table with cleaned temperature data\n",
    "df_join = df_imm.join(df_temp, df_imm.city==df_temp.city2, 'left')\n",
    "\n",
    "# final temperature table\n",
    "df_temp = df_join.selectExpr([\"city_id\",\n",
    "                              \"city\",\n",
    "                              \"Latitude AS latitude\",\n",
    "                              \"Longitude AS longitude\",\n",
    "                              \"avg_temp\"])\\\n",
    "                 .dropDuplicates(subset=['city'])\\\n",
    "                 .filter(df_join.avg_temp.isNotNull())\n",
    "\n",
    "# join final immigration table with cleaned demographics data\n",
    "df_join = df_imm.join(df_demographics, df_imm.city==df_demographics.city2, 'left')\n",
    "\n",
    "# final demographics table\n",
    "df_demographics = df_join.select([\"city_id\",\n",
    "                                  \"city\",\n",
    "                                  \"median_age\",\n",
    "                                  \"male_population\",\n",
    "                                  \"female_population\",\n",
    "                                  \"total_population\",\n",
    "                                  \"number_of_veterans\",\n",
    "                                  \"foreign_born\",\n",
    "                                  \"household_size\",\n",
    "                                  \"race\"])\\\n",
    "                 .dropDuplicates(subset=['city'])\\\n",
    "                 .filter(df_join.total_population.isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The data follows a star schema with one fact table and three dimensional tables. \n",
    "#### Fact table: immigration \n",
    "##### Data Fields: \n",
    "-\tid: a unique id for each entry (respondent). \n",
    "-\tyear: the year in which the respondent entered the US (all the data is for 2016). \n",
    "-\tmonth: the month in which the respondent entered the US (all the data is for April). \n",
    "-\torigin_country_id: and id for the country of origin of the respondent. The SAS description file provides the key-value pairs for these countries\n",
    "-\tport_city_id: the US port city, the city at which the respondent entered the US. This column will be used to join the data with the airport data to get the city name. \n",
    "-\tarrival_date: provides the date on which the respondent entered the US, includes year/month/day. \n",
    "-\ttransport_mode: a code to identify the mode of transport for the respondent: sea, air, land. \n",
    "-\tdeparture_date: a date field showing the respondentâ€™s departure date from the US, if departed already. \n",
    "-\tvisa_purpose: the purpose of the visa: business, study, tourism. \n",
    "-\tvisa_type: the US visa type: B1, B2, F1, and so on. \n",
    "-\tbirth_year: the year of birth of the respondent. \n",
    "-\tgender: the gender of the respondent. \n",
    "-\tadmin_number: the administration number for each entry and for each respondent.\n",
    "-\tairline_code: a code for the airline organization used by the respondent to travel, if travelling by air.\n",
    "-\tflight_number: the flight number that the respondent travelled in, if travelling by air. \n",
    "\n",
    "#### Dimension Tables: \n",
    "#### Table 1: airports\n",
    "##### Fields: \n",
    "-\tiata_code: a unique code for each airport consisting from three letters. \n",
    "-\tname: name of the airport. \n",
    "-\ttype: type of the airport; small, medium, large, etc. \n",
    "-\tcity: the name of the city in which the airport is located. \n",
    "-\tregion: the region in which the airport is located, it is mostly equivalent to the US state. -\tcountry: the country of the airport, in this case, it is the US. \n",
    "-\tcontinent: the continent in which the airport is located: North America. \n",
    "-\tcoordinates: the coordinate of the airport in northing and easting. \n",
    "-\tgps_code: a code for the location that is relevant for GPS systems.\n",
    "-\tlocal_code: a code for the airport as per US conventions. \n",
    "\n",
    "#### Table 2: temperatures \n",
    "##### Fields:\n",
    "-\tcity_id: a unique ID for each city, it is equivalent for the port_city_id in the facts table. \n",
    "-\tcity: the name of the city.\n",
    "-\tlatitude: the latitude of the city. \n",
    "-\tlongitude: the longitude of the city. \n",
    "-\tavg_temp: average monthly temperature of the city across all the years available. Original data has years 1750-2013. Since all arrivals are for April, then these averages are all for April. \n",
    "\n",
    "#### Table 3: demographics \n",
    "##### Fields:\n",
    "-\tcity_id: a unique ID for each city, it is equivalent for the port_city_id in the facts table. \n",
    "-\tcity: the name of the city.\n",
    "-\tmedian_age: median age of all people living in the city. \n",
    "-\tmale_population: number of male residents in the city. \n",
    "-\tfemale_population: number of female residents in the city. \n",
    "-\ttotal_population: number of total population male and female. \n",
    "-\tnumber_of_veteran: number of veterans in the city. \n",
    "-\tnumber_of_foreign_born: number of foreign-born residents. \n",
    "-\thousehold_size: the average household size in the city. \n",
    "-\tstate_code: the two-letter code for the state in which the city is located. \n",
    "-\trace: the majority race in the city. Explore Data: In this part, I will provide the count of each value for each data field. This will help us to understand the data and what it represents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
